architecture: 'lru' # transformer | hyena | lru

model:
  checkpoint_every_n: 2
  n_layers: 10
  d_model: 1024
  

optimizer:
  name: 'madgrad'
  args:
    lr: 4.0e-4

scheduler:
  warmup_steps: 2000

text_chunking:
  size: 8192

wandb:
  use: true
  project_name: "spotify_language_models"
  id: "" # leave empty if not resuming a previous run

checkpointing:
  dir: '/exp/exp4/acp21rjf/checkpoints/language_modelling_spotipile/lru/test'
  save_every_n_steps: 750
  
training:
  batch_size: 5
  backprop_every: 1

# size: 2048 = batch size 176
# size: 4096 = batch size 88
# size: 8192 = batch size 44
# size: 16384 = batch size 22
# size: 65536 = batch size 5 
# size: 131072 = batch size 2
# size: 360000 (1 hour) =  batch size 1
