architecture: 'mamba'

model:
  d_model: 1024
  n_layer: 8
  

optimizer:
  name: 'madgrad'
  args:
    lr: 1e-3

scheduler:
  warmup_steps: 2000

text_chunking:
  size: 4096

wandb:
  use: true
  project_name: "spotify_language_models"
  name: 'mamba_test'
  id: "" # leave empty if not resuming a previous run

checkpointing:
  dir: '/exp/exp4/acp21rjf/checkpoints/language_modelling_spotipile/mamba'
  save_every_n_steps: 750
  
training:
  batch_size: 12
  backprop_every: 1

# size: 2048 = batch size 176
# size: 4096 = batch size 88
# size: 8192 = batch size 44
# size: 16384 = batch size 22
# size: 65536 = batch size 5 
# size: 131072 = batch size 2
# size: 360000 (1 hour) =  batch size 1
