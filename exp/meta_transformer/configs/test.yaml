architecture: "meta_transformer"

model:
  dim: 512
  depth: 4
  heads: 8
  dim_head: 64
  temperature: 15.5
  shared_kv: true
  shared_temperture: false
  checkpoint_every_n: 0
  fused_mlp_checkpoint_lvl: 0
  

optimizer:
  name: 'madgrad'
  args:
    lr: 1e-5
    weight_decay: 0.1
    decouple_decay: true

scheduler:
  warmup_steps: 10000

text_chunking:
  size: 256

wandb:
  use: true
  project_name: "spotify_language_models"
  id: "" # leave empty if not resuming a previous run

checkpointing:
  dir: '/exp/exp4/acp21rjf/checkpoints/language_modelling_spotipile/meta_transformer/test'
  save_every_n_steps: 250
  
training:
  batch_size: 32
  backprop_every: 1
  max_seq_len: 256 # max cache length

# size: 2048 = batch size 176
# size: 4096 = batch size 88
# size: 8192 = batch size 44
# size: 16384 = batch size 22
# size: 65536 = batch size 5 
# size: 131072 = batch size 2
# size: 360000 (1 hour) =  batch size 1
